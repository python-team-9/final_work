# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'main.py'
#
# Created by: PyQt5 UI code generator 5.15.4
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.
import time
import requests
import re
import pymysql
from bs4 import BeautifulSoup
from threading import Timer
from datetime import datetime
def savedata(jobNames,jobCompanys,jobSalarys,jobPlaces,jobNumbers,jobEducations,jobExperiences, jobDescribes):
    db = pymysql.connect(host="47.99.201.114", user="root", passwd="Aa123456", db="jobOfferinformation", port=3306,charset="utf8")
    #db = pymysql.connect(host="localhost", user="root", passwd="Zyc64430628", db="jobOfferinformation", port=3306,charset="utf8")
    cursor = db.cursor()
    for i in range(len(jobNames)):
        cursor.execute(
            'INSERT INTO jobOfferDetail(jobName,jobCompany,jobSalary,jobPlace,jobDescribe,jobNumber,jobEducation,jobExperience) VALUES(%s,%s,%s,%s,%s,%s,%s,%s)',
            (jobNames[i].get_text(), jobCompanys[i].get_text(), jobSalarys[i].get_text(), jobPlaces[i].get_text(),jobDescribes[i].get_text() , jobNumbers[i].get_text(), jobEducations[i].get_text(), jobExperiences[i].get_text()))
        db.commit()
    db.close()

def getInformation(url):
    r = requests.get(url)  # 获取网站返回内容
    rt = r.content  # 获取文本
    rh = str(rt, "utf-8")  # 转为字符串，utf8格式
    soup = BeautifulSoup(rh, "html.parser")  # 使用soup获取

    jobNames = soup.find_all("a",attrs={"class" : "posName"})
    jobCompanys = soup.find_all("a",attrs={"class" : "entName"})
    jobSalarys = soup.find_all("li",attrs={"class" : "w3"})
    jobSalarys = jobSalarys[1:]
    jobPlaces = soup.find_all("li",attrs={"class" : "w4"})
    jobPlaces = jobPlaces[1:]
    jobQitas = soup.find_all("ul",attrs={"class" : "qitaUL"})
    jobNumbers = []
    jobEducations = []
    jobExperiences = []
    #jobNatures = []
    for jobQita in jobQitas:
        jobNumbers.append(jobQita.find("span"))
        jobEducations.append(jobQita.find_all("span")[1])
        jobExperiences.append(jobQita.find_all("span")[2])
        #jobNatures.append(jobQita.find_all("span")[3])

    #jobDate = soup.find_all("li",attrs={"class" : "w4"})
    #print(jobNatures)

    return jobNames,jobCompanys,jobSalarys,jobPlaces,jobNumbers,jobEducations,jobExperiences
def task():
    print(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

def getDetailInformation(jobNames):
    jobDescribes = []
    for jobName in jobNames:
        Timer(5,task,()).start()
        time.sleep(5)
        jobDescribe = getDetailInformationd(jobName)
        #print(jobDescribe.get_text())
        jobDescribes.append(jobDescribe)
    return jobDescribes

def getDetailInformationd(jobName):
    newurlp = str(jobName).find("target")
    newurl = "https:"+str(jobName)[25:newurlp-2]
    print(newurl)
    r = requests.get(newurl)  # 获取网站返回内容
    rt = r.content  # 获取文本
    rh = str(rt, "utf-8")  # 转为字符串，utf8格式
    soup = BeautifulSoup(rh, "html.parser")  # 使用soup获取

    #jobDetais = soup.find_all("p", attrs={"class": "detail"})
    # jobDetais = jobDetais[0].find_all("span")
    return  soup.find_all("pre", attrs={"id": "examineSensitiveWordsContent"})[0]
    #print(jobDetais[0])



jobNames,jobCompanys,jobSalarys,jobPlaces,jobNumbers,jobEducations,jobExperiences = getInformation("https://s.gxrc.com/sCareer?industry=0&listvalue=1&ordertype=0&pagesize=20&postype=5467&welfare=&page=1")
jobDescribes = getDetailInformation(jobNames)
savedata(jobNames,jobCompanys,jobSalarys,jobPlaces,jobNumbers,jobEducations,jobExperiences, jobDescribes)

